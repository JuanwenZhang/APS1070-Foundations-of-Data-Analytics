{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.6 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "F21_APS1070_Project_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "427dbe4e51b5387b95f6713934d34ee99dc52ee933d986e8e9d62b7e183bbc53"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTpnyUEJrAjE"
      },
      "source": [
        "# **Project 3**, APS1070 Fall 2021\n",
        "#### **PCA [10 marks]**\n",
        "**Deadline: Nov 5th, 21:00**\n",
        "\n",
        "**Academic Integrity**\n",
        "\n",
        "This project is individual - it is to be completed on your own. If you have questions, please post your query in the APS1070 Piazza Q&A forums (the answer might be useful to others!).\n",
        "\n",
        "Do not share your code with others, or post your work online. Do not submit code that you have not written yourself. Students suspected of plagiarism on a project, midterm or exam will be referred to the department for formal discipline for breaches of the Student Code of Conduct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUdJ6xw3rJIG"
      },
      "source": [
        "Please fill out the following:\n",
        "\n",
        "\n",
        "*   **Name**:\n",
        "*   **Student number**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwVsZkjMwA5N"
      },
      "source": [
        "In this project we work on a Covid-19 dataset that reports the number  cases for different countries at the end of each day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BHS-H5SwA5N"
      },
      "source": [
        "# Part 1: Getting started [1 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILjIVdnsXH2u"
      },
      "source": [
        "import pandas as pd\n",
        "cases_raw = pd.read_csv(\n",
        "    filepath_or_buffer='https://raw.githubusercontent.com/aps1070-2019/datasets/master/confirmed-june21.csv',\n",
        "    index_col=0,\n",
        "    thousands=','\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVOW-vqbwA5N"
      },
      "source": [
        "1. Write a function to do the following: **[0.25]**\n",
        "    * Takes the dataframe, and your country list as inputs (US, China, Canada, ...)\n",
        "    * Plots time-series for the input list (it is best to plot each country in a separate graph (subplot), so you can easily compare them.)\n",
        "    \n",
        "2. Apply `StandardScalar` to the data. Each day should have a `mean` of zero and a `StD` of 1. **[0.25]**\n",
        "3. Run the function in `step 1` on the standardized dataset for the `US`, `China`, and `Canada`.   **[0.25]**\n",
        "4. Discuss the trends in the standardized time-series for the `US`, `Canada`, and `China`. What does it mean if the curve goes up or down (are the number of covid cases negative?) What does the sign of values indicate? **[0.25]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9IN_qKNERbj"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4xcf_DpwA5N"
      },
      "source": [
        "# Part 2: Applying PCA [2 Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch7Ly5XQwA5N"
      },
      "source": [
        "1. Compute the covariance matrix of the dataframe. *Hint: The dimensions of your covariance matrix should be (511, 511).* **[0.25]**\n",
        "2. Write a function `get_sorted_eigen(df_cov)` that gets the covariance matrix of dataframe `df` (from step 1), and returns sorted eigenvalues and eigenvectors using `np.linalg.eigh`. **[0.25]**\n",
        "3. Show the effectiveness of your principal components in covering the variance of the dataset with a `scree plot`. **[0.25]**\n",
        "4. How many PCs do you need to cover 99% of the dataset's variance? **[0.25]**\n",
        "5. Plot the first 16 principal components (Eigenvectors) as a time series (16 subplots, on the x-axis you have dates and on the y-axis you have the value of the PC element) . **[0.5]**\n",
        "6. Compare the first few PCs with the rest of them. Do you see any difference in their trend? **[0.5]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2gQgts5ESF6"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmndIgPVwA5O"
      },
      "source": [
        "# Part 3: Data reconstruction [3 Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvtD5MnGwA5O"
      },
      "source": [
        "Create a function that:\n",
        "\n",
        "*   Accepts a country and the original dataset as inputs.\n",
        "*   Calls useful functions that you designed in previous parts to compute eigen vectors and eigen values. \n",
        "*   Plots 4 figures:\n",
        "  1.   The original time-series for the specified country. **[0.5]**\n",
        "  2.   The incremental reconstruction of the **original** (not standardized) time-series for the specified country in a single plot. **[1.5]**\n",
        "       * You should at least show 5 curves in a figure for incremental reconstruction. For example, you can pick the following (or any other combination that you think is reasonable): \n",
        "          * Reconstruction with only PC1\n",
        "          * Reconstruction with both PC1 and PC2\n",
        "          * Reconstruction with PC1 to PC4 (First 4 PCs)\n",
        "          * Reconstruction with PC1 to PC8 (First 8 PCs)\n",
        "          * Reconstruction with PC1 to PC16 (First 16 PCs)\n",
        "\n",
        "      * Hint: you need to compute the reconstruction for the standardized time-series first, and then scale it back to the original (non-standardized form) using the StandardScaler `inverse_transform` [help...](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.inverse_transform)\n",
        "  3.   The residual error for your best reconstruction with respect to the original time-series. **[0.5]**\n",
        "      * Hint: You are plotting the error that we have for reconstructing each day `(df - df_reconstructed)`. On the x-axis, you have dates, and on the y-axis, the residual error. \n",
        "  4.   The RMSE of the reconstruction as a function of the number of included components (x-axis is the number of components and y-axis is the RMSE). Sweep x-axis from 1 to 10 (this part is independent from part 3.2.) **[1]**\n",
        "\n",
        "Test your function using the `US`, `Canada`, and `China` as inputs. **[0.5]**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKXBlzSOESoE"
      },
      "source": [
        "def plot_country_figures(original_df, country_name):\n",
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4orcdPaCQup"
      },
      "source": [
        "# Part 4: SVD [2 Marks]\n",
        "Modify your code in part 3 to use SVD instead of PCA for extracting the eigenvectors. **[1]**\n",
        "\n",
        "Explain if standardization or covariance computation is required for this part.\n",
        "Repeat part 3 and compare your PCA and SVD results. **[1]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBXJVyTVDWiX"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsrjEsv6Hr_6"
      },
      "source": [
        "# Part 5: Let's collect a more recent dataset! [2 Marks]\n",
        "Create a more recent dataset similar to the one provided in your handout using the raw information provided [here](https://datahub.io/core/covid-19#resource-time-series-19-covid-combined). **[1]**\n",
        "\n",
        "You need to manipulate the data to organize it in the desired format. You are free to use any tools you like, from Excel to Python!\n",
        " \n",
        "In the end, you should have a new CSV file with more dates (features) compared to the provided dataset. \n",
        "\n",
        "\n",
        "Upload your new dataset (in CSV format) to your colab notebook and repeat part 4. **[1]**\n",
        "\n",
        "\n",
        "Don't forget to add your new CSV file to your GitHub repo. The code below helps you to upload your new CSV file to your colab session. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe5y0ozcLtox"
      },
      "source": [
        "# load train.csv to Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rKZXBodwA5P"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA-T9S4RwA5P"
      },
      "source": [
        "Understanding PCA and SVD:\n",
        "\n",
        "1. https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8\n",
        "\n",
        "2. https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca\n",
        "\n",
        "3. https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
        "\n",
        "4. https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/\n",
        "\n",
        "PCA:\n",
        "\n",
        "1. Snippets from: https://plot.ly/ipython-notebooks/principal-component-analysis/\n",
        "\n",
        "2. https://www.value-at-risk.net/principal-component-analysis/\n",
        "\n",
        "Covid Data:\n",
        "\n",
        "1. https://www.worldometers.info/coronavirus/\n",
        "\n",
        "2. https://datahub.io/core/covid-19#resource-time-series-19-covid-combined\n",
        "\n"
      ]
    }
  ]
}